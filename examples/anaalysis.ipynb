{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 分析数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tokenizer_path = \"/home/wangyuxin//.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots\"\n",
    "# data_path = '/home/nus-hx/code/Sequence-Scheduling/data/alpaca-train-10k.json'\n",
    "tokenizer_path = \"/home/nus-hx/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a\"\n",
    "data_path = '/home/nus-hx/code/Sequence-Scheduling/data/alpaca-train-10k.json'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "print(tokenizer)\n",
    "def load_json(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "alpaca_data = load_json(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991467a6",
   "metadata": {},
   "source": [
    "## 1.1 alpaca数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a26abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(alpaca_data)\n",
    "data = {}\n",
    "lengths = []\n",
    "tokens = []\n",
    "for i in range(num_samples):\n",
    "    prompt = alpaca_data[i]['conversations'][0]['value']\n",
    "    ids = tokenizer.encode(prompt)\n",
    "    data[i] = {\n",
    "        'prompt': prompt,\n",
    "        'length': len(prompt.split(' ')),    \n",
    "        'num_tokens': len(ids)    \n",
    "    }\n",
    "    lengths.append(len(prompt.split(' ')))\n",
    "    tokens.append(len(ids))\n",
    "print('text length', np.min(lengths), np.mean(lengths), np.max(lengths))\n",
    "print('#tokens', np.min(tokens), np.mean(tokens), np.max(tokens))\n",
    "\n",
    "\n",
    "# # 示例数据：替换成您的实际数据\n",
    "# lengths = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "\n",
    "# 对长度进行排序\n",
    "sorted_lengths = sorted(tokens)\n",
    "\n",
    "# 绘制直方图\n",
    "plt.hist(sorted_lengths, bins=100, edgecolor='black')\n",
    "\n",
    "# 添加标题和标签\n",
    "# plt.title('Token Length Distribution')\n",
    "plt.xlabel('#Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b577d00",
   "metadata": {},
   "source": [
    "## 1.2 Yizhong 数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yizhongw/self_instruct\", \"super_natural_instructions\")\n",
    "data_prompts = dataset['train']['prompt']\n",
    "num_samples = len(data_prompts)\n",
    "data = {}\n",
    "lengths = []\n",
    "tokens = []\n",
    "for i in range(num_samples):\n",
    "    prompt = data_prompts[i]\n",
    "    ids = tokenizer.encode(prompt)\n",
    "    data[i] = {\n",
    "        'prompt': prompt,\n",
    "        'length': len(prompt.split(' ')),    \n",
    "        'num_tokens': len(ids)    \n",
    "    }\n",
    "    lengths.append(len(prompt.split(' ')))\n",
    "    tokens.append(len(ids))\n",
    "print('text length', np.min(lengths), np.mean(lengths), np.max(lengths))\n",
    "print('#tokens', np.min(tokens), np.mean(tokens), np.max(tokens))\n",
    "\n",
    "# # 示例数据：替换成您的实际数据\n",
    "# lengths = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "\n",
    "# 对长度进行排序\n",
    "sorted_lengths = sorted(tokens)\n",
    "\n",
    "# 绘制直方图\n",
    "plt.hist(sorted_lengths, bins=100, edgecolor='black')\n",
    "\n",
    "# 添加标题和标签\n",
    "# plt.title('Token Length Distribution')\n",
    "plt.xlabel('#Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda952c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1addd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0ebdfa",
   "metadata": {},
   "source": [
    "## 1.3 unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# data = load_json('/home/nus-hx/code/ColossalAI/examples/language/openmoe/expert_input_statistics_yizhongw.json')\n",
    "# data = load_json('/home/nus-hx/code/ColossalAI/examples/language/openmoe/expert_input_statistics.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(\"/home/nus-hx/code/Sequence-Scheduling/data/alpaca-train-10k.json\")\n",
    "data[1]['conversations'][0]['value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    all_num_tokens_per_expert = []\n",
    "    # all_id_value_per_expert = []\n",
    "    for layer_id, layer_expert_info in data.items():\n",
    "        all_num_tokens_per_expert.append([])\n",
    "        # all_id_value_per_expert.append([])\n",
    "        for expert_idx, expert_info in layer_expert_info.items():\n",
    "            all_num_tokens_per_expert[-1].append(expert_info[1:])\n",
    "            # all_id_value_per_expert[-1].append(expert_info['id_value_per_expert'])\n",
    "    return torch.tensor(all_num_tokens_per_expert)\n",
    "    # return torch.tensor(all_num_tokens_per_expert), torch.tensor(all_id_value_per_expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eceada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_num_tokens(num_tokens, stpes=5):\n",
    "    num_layers, num_experts, num_steps = num_tokens.shape\n",
    "    num_steps = min(stpes, num_steps)\n",
    "    # 创建一个子图，每个步骤一个子图\n",
    "    fig, axs = plt.subplots(num_steps, 1, figsize=(num_experts, num_layers * num_steps))\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        matrix = num_tokens[:, :, step]\n",
    "\n",
    "        # 绘制热图\n",
    "        im = axs[step].imshow(matrix, cmap='viridis')\n",
    "\n",
    "        # 显示颜色条\n",
    "        cbar = axs[step].figure.colorbar(im, ax=axs[step])\n",
    "\n",
    "        # 设置坐标轴标签\n",
    "        axs[step].set_xticks(np.arange(num_experts))\n",
    "        axs[step].set_yticks(np.arange(num_layers))\n",
    "        axs[step].set_xticklabels([f'Expert {i}' for i in range(num_experts)])\n",
    "        axs[step].set_yticklabels([f'Layer {i}' for i in range(num_layers)])\n",
    "\n",
    "        # 在矩阵元素上显示数值\n",
    "        for i in range(num_layers):\n",
    "            for j in range(num_experts):\n",
    "                axs[step].text(j, i, f'{matrix[i, j]:.2f}', ha='center', va='center', color='w')\n",
    "\n",
    "        axs[step].set_title(f'Step {step + 1}')\n",
    "\n",
    "    # 调整子图布局\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def kl_divergence(matrix1, matrix2, eps=1e-10):\n",
    "    \"\"\"\n",
    "    计算两个矩阵每一行数据的KL散度（PyTorch版本）\n",
    "    \"\"\"\n",
    "    # 将每一行的数据归一化为概率分布epsilon = 1e-10  # 或者选择适当的小值\n",
    "    distributions1 = (matrix1 + eps) / torch.sum(matrix1 + eps, dim=1, keepdim=True)\n",
    "    distributions2 = (matrix2 + eps) / torch.sum(matrix2 + eps, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "    # 避免对数计算中出现无穷大值\n",
    "    distributions1 = torch.where(torch.isnan(distributions1), torch.tensor(0.0), distributions1)\n",
    "    distributions2 = torch.where(torch.isnan(distributions2), torch.tensor(0.0), distributions2)\n",
    "\n",
    "    # 计算KL散度\n",
    "    kl = torch.sum(distributions1 * torch.log(distributions1 / distributions2), dim=1)\n",
    "\n",
    "    return kl\n",
    "\n",
    "\n",
    "def jensen_shannon_divergence(matrix1, matrix2, epsilon=1e-10):\n",
    "    # 将每一行的数据归一化为概率分布\n",
    "    distributions1 = (matrix1 + epsilon) / (torch.sum(matrix1, dim=1, keepdim=True) + epsilon)\n",
    "    distributions2 = (matrix2 + epsilon) / (torch.sum(matrix2, dim=1, keepdim=True) + epsilon)\n",
    "\n",
    "    # 计算平均分布\n",
    "    average_distribution = 0.5 * (distributions1 + distributions2)\n",
    "\n",
    "    # 计算KL散度\n",
    "    kl_divergence1 = torch.sum(distributions1 * torch.log(distributions1 / (average_distribution + epsilon)), dim=1)\n",
    "    kl_divergence2 = torch.sum(distributions2 * torch.log(distributions2 / (average_distribution + epsilon)), dim=1)\n",
    "\n",
    "    # 计算Jensen-Shannon散度\n",
    "    js_divergence = 0.5 * (kl_divergence1 + kl_divergence2)\n",
    "    similarity = 1 - 2 * js_divergence\n",
    "    return js_divergence, similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cache\n",
    "data = load_json('/home/nus-hx/code/vllm/examples/4_expert_input_statistics.json')\n",
    "num_tokens = preprocess(data) # (num_layers, num_experts, num_steps)\n",
    "prefilling = num_tokens[..., 0]\n",
    "decoding = num_tokens[..., 1:].sum(-1)\n",
    "js, sim = jensen_shannon_divergence(prefilling, decoding)\n",
    "# print(f\"prefilling #tokens: {prefilling}\")\n",
    "# print(f\"decoding #tokens: {decoding}\")\n",
    "print(f\"js: {js.numpy()} 相似度: {sim}\\n\")\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# 创建两个示例矩阵\n",
    "matrix1 = prefilling\n",
    "matrix2 = decoding\n",
    "\n",
    "# 计算Pearson相关系数\n",
    "pearson_corr, _ = pearsonr(matrix1.flatten(), matrix2.flatten())\n",
    "print(f\"Pearson Correlation: {pearson_corr}\")\n",
    "\n",
    "# 计算Spearman等级相关系数\n",
    "spearman_corr, _ = spearmanr(matrix1.flatten(), matrix2.flatten())\n",
    "print(f\"Spearman Rank Correlation: {spearman_corr}\")\n",
    "\n",
    "print(sim.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c70b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_tokens(num_tokens, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    data = load_json(f'/home/nus-hx/code/ColossalAI/examples/language/openmoe/{i}_expert_input_statistics.json')\n",
    "    num_tokens, id_values = preprocess(data) # (num_layers, num_experts, num_steps)\n",
    "    prefilling = num_tokens[..., 0]\n",
    "    decoding = num_tokens[..., 1:].sum(-1)\n",
    "    js, sim = jensen_shannon_divergence(prefilling, decoding)\n",
    "    print(f\"{i}-th sample:\")\n",
    "    print(f\"prefilling #tokens: {prefilling}\")\n",
    "    print(f\"decoding #tokens: {decoding}\")\n",
    "    print(f\"js: {js.numpy()} 相似度: {sim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_tokens(num_tokens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec9cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3ef7636",
   "metadata": {},
   "source": [
    "## use cache\n",
    "data = load_json('/home/nus-hx/code/ColossalAI/examples/language/openmoe/expert_input_statistics_no_cache.json')\n",
    "num_tokens, id_values = preprocess(data)\n",
    "for step in range(10):\n",
    "    print(num_tokens[0,:,step].numpy(), num_tokens[0,:,step].numpy().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43750eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8a875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    id_means = []\n",
    "    id_stds = []\n",
    "    for layer_id, layer_expert_info in data.items():\n",
    "        id_means.append([])\n",
    "        id_stds.append([])\n",
    "        for expert_idx, expert_info in layer_expert_info.items():\n",
    "            id_means[-1].append(np.mean(expert_info))\n",
    "            id_stds[-1].append(np.std(expert_info))\n",
    "    id_means, id_stds = np.array(id_means), np.array(id_stds)\n",
    "#     print('means:\\n', id_means)\n",
    "#     print('stds:\\n', id_stds)\n",
    "    return id_means, id_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mean_variance(matrix_mean, matrix_var, is_sort=True):\n",
    "    if is_sort:\n",
    "        # 对均值矩阵按行排序\n",
    "        sorted_indices = np.argsort(matrix_mean, axis=1)\n",
    "        sorted_matrix_mean = np.take_along_axis(matrix_mean, sorted_indices, axis=1)\n",
    "        sorted_matrix_var = np.take_along_axis(matrix_var, sorted_indices, axis=1)\n",
    "    else:\n",
    "        sorted_matrix_mean = matrix_mean\n",
    "        sorted_matrix_var = matrix_var\n",
    "\n",
    "    num_layers, num_experts = sorted_matrix_mean.shape\n",
    "\n",
    "    plt.figure(figsize=(10, 6 * num_layers))  # 设置图表尺寸，每一层一个子图\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        plt.subplot(num_layers, 1, layer + 1)  # 创建子图\n",
    "        mean_values = sorted_matrix_mean[layer]  # 获取当前层的均值\n",
    "        var_values = sorted_matrix_var[layer]  # 获取当前层的方差\n",
    "\n",
    "        # 绘制图表，均值用条形图表示，方差用error bar表示\n",
    "        plt.bar(np.arange(num_experts), mean_values, yerr=var_values, capsize=5)\n",
    "        plt.xlabel('Expert Index')\n",
    "        plt.ylabel('Mean Value')\n",
    "        plt.title(f'Layer {layer + 1}')\n",
    "\n",
    "    plt.tight_layout()  # 调整布局\n",
    "    plt.show()\n",
    "\n",
    "# # 测试示例\n",
    "# num_layer = 4\n",
    "# num_expert = 6\n",
    "# matrix_mean = np.random.randint(1, 50, size=(num_layer, num_expert))  # 生成随机均值矩阵\n",
    "# matrix_var = np.random.randint(1, 20, size=(num_layer, num_expert))  # 生成随机方差矩阵\n",
    "# print(\"原始均值矩阵：\")\n",
    "# print(matrix_mean)\n",
    "# print(\"原始方差矩阵：\")\n",
    "# print(matrix_var)\n",
    "# plot_mean_variance(matrix_mean, matrix_var, is_sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c0dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yizhongw = preprocess(load_json('/home/nus-hx/code/ColossalAI/examples/language/openmoe/expert_input_statistics_yizhongw.json'))\n",
    "# data_wikitext = preprocess(load_json('/home/nus-hx/code/ColossalAI/examples/language/openmoe/expert_input_statistics_wikitext.json'))\n",
    "data_wikitext = preprocess(load_json('/home/nus-hx/code/ColossalAI/examples/language/openmoe/expert_input_statistics.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_variance(*data_yizhongw, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe180f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_variance(*data_wikitext, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08c8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f191b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_expert_values(matrix, to_sort=False):\n",
    "    sorted_matrix = matrix\n",
    "    if to_sort:\n",
    "        sorted_matrix = np.sort(matrix, axis=1)  # 对矩阵按行排序\n",
    "        print(f\"排序后矩阵：\\n\", sorted_matrix)\n",
    "\n",
    "    num_layers, num_experts = sorted_matrix.shape\n",
    "\n",
    "    plt.figure(figsize=(10, 6 * num_layers))  # 设置图表尺寸，每一层一个子图\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        plt.subplot(num_layers, 1, layer + 1)  # 创建子图\n",
    "        values = sorted_matrix[layer]  # 获取当前层的数值\n",
    "\n",
    "        # 绘制图表\n",
    "        plt.bar(np.arange(num_experts), values)\n",
    "        plt.xlabel('Expert Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'Layer {layer + 1}')\n",
    "\n",
    "    plt.tight_layout()  # 调整布局\n",
    "    plt.show()\n",
    "\n",
    "# 测试示例\n",
    "num_layer = 4\n",
    "num_expert = 6\n",
    "matrix = np.random.randint(1, 50, size=(num_layer, num_expert))  # 生成随机矩阵\n",
    "\n",
    "\n",
    "# results on yizhongw\n",
    "matrix = [   \n",
    "    [0,0,5.544073,10.722609,6.840683,3.627592,4.612808,4.135404,4.916871,6.619530,6.562585,5.101259,5.618176,6.037780,6.207156,5.236032],\n",
    "    [0,0,4.355231,5.640529,7.701429,8.725691,11.453165,9.376528,6.904047,5.065467,6.500171,8.404157,5.460723,4.534373,6.314137,8.272322],\n",
    "    [0,0,5.698514,10.105216,8.807961,6.142078,6.140738,7.836411,10.673695,5.086765,7.178852,5.035629,3.078635,6.750798,9.594976,8.384435]\n",
    "]\n",
    "matrix = np.array(matrix)\n",
    "matrix = np.nan_to_num(matrix)\n",
    "print(\"原始矩阵：\")\n",
    "print(matrix)\n",
    "plot_expert_values(matrix, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeddb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [\n",
    "    [0,0,5.544073,10.722609,6.840683,3.627592,4.612808,4.135404,4.916871,6.619530,6.562585,5.101259,5.618176,6.037780,6.207156,5.236032],\n",
    "    [0,0,4.355231,5.640529,7.701429,8.725691,11.453165,9.376528,6.904047,5.065467,6.500171,8.404157,5.460723,4.534373,6.314137,8.272322],\n",
    "    [0,0,5.698514,10.105216,8.807961,6.142078,6.140738,7.836411,10.673695,5.086765,7.178852,5.035629,3.078635,6.750798,9.594976,8.384435],\n",
    "]\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "matrix = np.nan_to_num(matrix)\n",
    "print(\"原始矩阵：\")\n",
    "print(matrix)\n",
    "plot_expert_values(matrix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce481f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186eedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed7e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results on Wikitext-2-v1\n",
    "\n",
    "matrix = [\n",
    "    [0.000000,0.000000,7.963334,7.248610,5.389153,5.796610,4.887456,5.961907,5.427928,5.641143,3.553481,4.450169,6.972357,4.594432,8.926065,7.169308],\n",
    "[0.000000,10.373790,6.114333,11.607411,11.788904,12.568120,9.828843,9.701502,9.960600,5.349957,8.947469,11.169651,6.741775,8.415722,8.856209,8.982792],\n",
    "[0.000000,0.000000,5.875930,7.778467,9.251879,11.587394,13.382789,10.927470,10.783409,9.167251,11.630848,9.831821,13.274390,6.963855,12.515475,8.359204]\n",
    "]\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "matrix = np.nan_to_num(matrix)\n",
    "print(\"原始矩阵：\")\n",
    "print(matrix)\n",
    "plot_expert_values(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a880df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_expert_values(matrix, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706333a6",
   "metadata": {},
   "source": [
    "# 1. 导入依赖库和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "num_samples = 10000\n",
    "# pt = torch.load(f'../alpaca_{num_samples}_samples.pt')\n",
    "pt = torch.load(f'../yizhong_{num_samples}_samples.pt')\n",
    "type(pt), len(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 定义数据解析函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b744a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prompt_expert_trace(seq_group):\n",
    "    t2e = seq_group.token2experts\n",
    "    seq = seq_group.get_seqs()[0]\n",
    "    num_layers = len(t2e[0]) - 1\n",
    "    num_experts = 8\n",
    "    expert_trace = torch.zeros((num_layers, num_experts)).to(torch.cuda.current_device())\n",
    "    prompt_len = seq.get_prompt_len()\n",
    "    for i_token in range(prompt_len):\n",
    "        for layer_id in t2e[i_token]:\n",
    "            if isinstance(layer_id, int):\n",
    "                experts = t2e[i_token][layer_id][0]\n",
    "                for expert_id in experts:\n",
    "                    expert_trace[layer_id][expert_id] += 1\n",
    "    return expert_trace\n",
    "\n",
    "def get_output_expert_trace(seq_group):\n",
    "    t2e = seq_group.token2experts\n",
    "    seq = seq_group.get_seqs()[0]\n",
    "    num_layers = len(t2e[0]) - 1\n",
    "    num_experts = 8\n",
    "    expert_trace = torch.zeros((num_layers, num_experts)).to(torch.cuda.current_device())\n",
    "    prompt_len = seq.get_prompt_len()\n",
    "    output_len = len(seq.get_output_token_ids())\n",
    "    all_length = len(seq.get_token_ids())\n",
    "    # print(f\"all:{all_length}=prompt({prompt_len}) + output({output_len})\")\n",
    "    assert prompt_len+output_len == all_length\n",
    "    for i_token in range(prompt_len, all_length-1):\n",
    "        for layer_id in t2e[i_token]:\n",
    "            if isinstance(layer_id, int):\n",
    "                experts = t2e[i_token][layer_id][0]\n",
    "                for expert_id in experts:\n",
    "                    expert_trace[layer_id][expert_id] += 1\n",
    "    return expert_trace\n",
    "\n",
    "def parse_seq_group(seq_group):\n",
    "    prompt_expert_trace = get_prompt_expert_trace(seq_group)\n",
    "    output_expert_trace = get_output_expert_trace(seq_group)\n",
    "    seq = seq_group.get_seqs()[0]\n",
    "    token_ids = seq.get_token_ids()\n",
    "    prompt_len = seq.get_prompt_len()\n",
    "    prompt_ids = token_ids[:prompt_len]\n",
    "    output_ids = seq.get_output_token_ids()\n",
    "    output_len = len(output_ids)\n",
    "    return {\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'output_ids': output_ids,\n",
    "        'token_ids': token_ids,\n",
    "        'prompt_len': prompt_len,\n",
    "        'output_len': output_len,\n",
    "        'prompt_expert_trace': prompt_expert_trace,\n",
    "        'output_expert_trace': output_expert_trace\n",
    "    }\n",
    "\n",
    "def get_token_expert_trace(per_token2experts):\n",
    "    trace = []\n",
    "    for i in range(len(per_token2experts)-1):\n",
    "        trace.append(per_token2experts[i][0])\n",
    "    return torch.from_numpy(np.array(trace))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49364932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_group = pt[0].outputs[0].seq_group\n",
    "token2experts = seq_group.token2experts\n",
    "seq = seq_group.get_seqs()[0]\n",
    "\n",
    "data = parse_seq_group(seq_group)\n",
    "data.keys(), data['prompt_expert_trace'][:5], get_token_expert_trace(seq_group.token2experts[0])[:5]\n",
    "\n",
    "token_ids = seq.get_token_ids()\n",
    "output_ids = seq.get_output_token_ids()\n",
    "prompt_len = seq.get_prompt_len()\n",
    "print(f'length: {len(token_ids)}={prompt_len}+{len(output_ids)}')\n",
    "print('All:', token_ids)\n",
    "print('prompt:', token_ids[:prompt_len])\n",
    "print('output:', output_ids)\n",
    "unique_ids = []\n",
    "repeat_ids = {}\n",
    "for i, idx in enumerate(token_ids):\n",
    "    if idx not in unique_ids:\n",
    "        unique_ids.append(idx)\n",
    "    else:\n",
    "        if idx not in repeat_ids:\n",
    "            repeat_ids[idx] = [i]\n",
    "        else:\n",
    "            repeat_ids[idx].append(i)\n",
    "print('unique:', f\"{len(unique_ids)}/{seq.get_len()}\")\n",
    "print('repeat:', repeat_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986e2a3",
   "metadata": {},
   "source": [
    "# 3. 解析数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 分析单个 token 的 expert_trace\n",
    "\n",
    "查看分析单个 token 的 expert_trace是否与上下文无关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算指定 token 的 expert_trace (L,2)在不同上下文的相似性\n",
    "def get_consistency_score_token_expert_trace(t2e_pool, token_idx: int):\n",
    "    data = torch.stack( t2e_pool[token_idx]['trace'] ).sort(-1)\n",
    "    scores = torch.zeros(len(data), len(data))\n",
    "    for i in range(len(data)):\n",
    "        scores[i,i] = 1.\n",
    "        for j in range(i+1, len(data)):\n",
    "            tet1 = data[i].sort()[0]\n",
    "            tet2 = data[j].sort()[0]\n",
    "            score = (tet1==tet2).sum() / tet1.numel()\n",
    "            scores[i,j] = score\n",
    "            scores[j,i] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38968c94",
   "metadata": {},
   "source": [
    "### Prefilling和Decoding整体上下文情况下token 的 expert_trace (L,2)在不同上下文的相似性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2e_pool = {\n",
    "    # 'token_id': {\n",
    "    #     'trace': [\n",
    "    #         [token_expert_trace], [token_expert_trace]\n",
    "    #         ],\n",
    "    #         'position': [18, 19]\n",
    "    #     }\n",
    "    # },\n",
    "}\n",
    "seq_group_list = [pt[i].outputs[0].seq_group for i in range(len(pt))]\n",
    "for seq_group in seq_group_list:\n",
    "    crt_t2e = seq_group.token2experts\n",
    "    for i in range(len(crt_t2e)):\n",
    "        crt_token_t2e = crt_t2e[i]\n",
    "        token_idx = crt_token_t2e['token_idx']\n",
    "        token_expert_trace = get_token_expert_trace(crt_token_t2e)\n",
    "        if token_idx not in t2e_pool:\n",
    "            t2e_pool[token_idx] = {'position': [], 'trace': []}\n",
    "        t2e_pool[token_idx]['position'].append(i)\n",
    "        t2e_pool[token_idx]['trace'].append(token_expert_trace)\n",
    "print(len(t2e_pool), t2e_pool.keys())\n",
    "token_expert_trace_consistence = [\n",
    "    # token_idx, score\n",
    "]\n",
    "for i, token_idx in enumerate(t2e_pool):\n",
    "    if len(t2e_pool[token_idx]) > 1:\n",
    "        score = get_consistency_score_token_expert_trace(t2e_pool, token_idx).mean()\n",
    "        token_expert_trace_consistence.append([token_idx, score])\n",
    "np.array(token_expert_trace_consistence)[:10:20].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaed75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mean_max_std = lambda x: (x.min(), x.mean(), x.max(), x.std())\n",
    "min_mean_max_std(np.array(token_expert_trace_consistence)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61234c8",
   "metadata": {},
   "source": [
    "### Prefilling上下文情况下token 的 expert_trace (L,2)在不同上下文的相似性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2e_pool = {\n",
    "    # 'token_id': {\n",
    "    #     'trace': [\n",
    "    #         [token_expert_trace], [token_expert_trace]\n",
    "    #         ],\n",
    "    #         'position': [18, 19]\n",
    "    #     }\n",
    "    # },\n",
    "}\n",
    "seq_group_list = [pt[i].outputs[0].seq_group for i in range(len(pt))]\n",
    "for seq_group in seq_group_list:\n",
    "    crt_t2e = seq_group.token2experts\n",
    "    seq = seq_group.get_seqs()[0]\n",
    "    prompt_len = seq.get_prompt_len()\n",
    "    for i in range(prompt_len):\n",
    "        crt_token_t2e = crt_t2e[i]\n",
    "        token_idx = crt_token_t2e['token_idx']\n",
    "        token_expert_trace = get_token_expert_trace(crt_token_t2e)\n",
    "        if token_idx not in t2e_pool:\n",
    "            t2e_pool[token_idx] = {'position': [], 'trace': []}\n",
    "        t2e_pool[token_idx]['position'].append(i)\n",
    "        t2e_pool[token_idx]['trace'].append(token_expert_trace)\n",
    "print(len(t2e_pool), t2e_pool.keys())\n",
    "token_expert_trace_consistence = [\n",
    "    # token_idx, min_score, mean_score\n",
    "]\n",
    "for token_idx in t2e_pool:\n",
    "    if len(t2e_pool[token_idx]) > 1:\n",
    "        scores = get_consistency_score_token_expert_trace(t2e_pool, token_idx)\n",
    "        token_expert_trace_consistence.append([token_idx, scores.min().item(), scores.mean().item()])\n",
    "token_expert_trace_consistence = np.array(token_expert_trace_consistence)\n",
    "min_mean_max_std = lambda x: (x.min(), x.mean(), x.max(), x.std())\n",
    "min_mean_max_std(np.array(token_expert_trace_consistence)[:,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3ca11",
   "metadata": {},
   "source": [
    "### Decoding 上下文情况下token 的 expert_trace (L,2)在不同上下文的相似性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2e_pool = {\n",
    "    # 'token_id': [\n",
    "    #     [1st token_exper_trace],\n",
    "    #     [2nd token_exper_trace],\n",
    "    #     ...\n",
    "    # ],\n",
    "}\n",
    "seq_group_list = [pt[i].outputs[0].seq_group for i in range(len(pt))]\n",
    "for seq_group in seq_group_list:\n",
    "    crt_t2e = seq_group.token2experts\n",
    "    seq = seq_group.get_seqs()[0]\n",
    "    token_ids = seq.get_token_ids()\n",
    "    output_ids = seq.get_output_token_ids()\n",
    "    prompt_len = seq.get_prompt_len()\n",
    "    for i in range(prompt_len, len(token_ids)-1):\n",
    "        crt_token_t2e = crt_t2e[i]\n",
    "        token_idx = crt_token_t2e['token_idx']\n",
    "        token_expert_trace = get_token_expert_trace(crt_token_t2e)\n",
    "        if token_idx not in t2e_pool:\n",
    "            t2e_pool[token_idx] = {'position': [], 'trace': []}\n",
    "        t2e_pool[token_idx]['position'].append(i)\n",
    "        t2e_pool[token_idx]['trace'].append(token_expert_trace)\n",
    "print(len(t2e_pool), t2e_pool.keys())\n",
    "token_expert_trace_consistence = [\n",
    "    # token_idx, min_score, mean_score\n",
    "]\n",
    "for token_idx in t2e_pool:\n",
    "    if len(t2e_pool[token_idx]) > 1:\n",
    "        scores = get_consistency_score_token_expert_trace(t2e_pool, token_idx)\n",
    "        token_expert_trace_consistence.append([token_idx, scores.min().item(), scores.mean().item()])\n",
    "token_expert_trace_consistence = np.array(token_expert_trace_consistence)\n",
    "min_mean_max_std = lambda x: (x.min(), x.mean(), x.max(), x.std())\n",
    "min_mean_max_std(np.array(token_expert_trace_consistence)[:,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee893e35",
   "metadata": {},
   "source": [
    "## 3.2 prefilling expert_trace 预测准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data_list = []\n",
    "failed_list = []\n",
    "seq_group_list = [pt[i].outputs[0].seq_group for i in range(len(pt))]\n",
    "for seq_idx, seq_group in enumerate(seq_group_list):\n",
    "    try:\n",
    "        data = parse_seq_group(seq_group)\n",
    "        seq_data_list.append(data)\n",
    "    except Exception as e:\n",
    "        print(seq_idx, str(e))\n",
    "        failed_list.append(seq_group)\n",
    "len(seq_data_list), len(failed_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids_list = np.array([data['prompt_ids'] for data in seq_data_list])\n",
    "prompt_trace_lis = [data['prompt_expert_trace'] for data in seq_data_list]\n",
    "output_ids_list = np.array([data['output_ids'] for data in seq_data_list])\n",
    "output_trace_lis = [data['output_expert_trace'] for data in seq_data_list]\n",
    "prompt_trace_lis[0], output_trace_lis[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4871a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(prompt_ids_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_infos = {}\n",
    "for seq_idx in range(len(pt)):\n",
    "    t2e = pt[seq_idx].outputs[0].seq_group.token2experts\n",
    "    for token_idx in t2e:\n",
    "        token_index = t2e[token_idx]['token_idx']\n",
    "        expert_trace_info ={}\n",
    "        for key in t2e[token_idx]:\n",
    "            if key != 'token_idx':\n",
    "                expert_trace_info[key] = t2e[token_idx][key][0]\n",
    "        if token_index not in token_infos:\n",
    "            token_infos[token_index] = [[expert_trace_info, token_idx]]\n",
    "        else:\n",
    "            token_infos[token_index].append([expert_trace_info, token_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf868323",
   "metadata": {},
   "outputs": [],
   "source": [
    "_token_num = {}\n",
    "for key in token_infos:\n",
    "    num = len(token_infos[key])\n",
    "    if num not in _token_num:\n",
    "        _token_num[num] = [key]\n",
    "    else:\n",
    "       _token_num[num].append(key)\n",
    "token_num  =_token_num\n",
    "# token_num = sorted(_token_num.items(), key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys  = np.array(sorted(list(token_num.keys())))\n",
    "print(keys, len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbea83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_num[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd91ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idx = 2757\n",
    "num = min(10, len(token_infos[token_idx]))\n",
    "token_indices_in_prompts = [token_infos[token_idx][t_idx][1] for t_idx in range(num)]\n",
    "expert_choices = np.hstack(\n",
    "    [\n",
    "        [ token_infos[token_idx][t_idx][0][i] for i in range(10)] for t_idx in range(num)\n",
    "    ]\n",
    ")\n",
    "print(token_indices_in_prompts, '\\n', expert_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/nus-hx/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torch.tensor(\n",
    "    [[11., 12., 17., 10.,  8.,  8.,  5., 19., 12.,  8., 13., 11.,  8., 19., 13.,  6.],\n",
    "    [15., 16., 20., 19., 14., 13., 10., 25., 13., 12., 15., 18., 18., 29., 20.,  7.],\n",
    "    [11., 14., 15., 17.,  9., 12.,  9., 23., 18., 12., 14., 15., 10., 21., 13.,  7.],\n",
    "    [13., 18., 22., 20., 19., 10.,  7., 27., 14., 15., 15., 16., 20., 30., 18.,  8.],\n",
    "    [ 9., 11., 15., 12., 10., 10.,  8., 17., 15., 11., 14., 13.,  9., 16., 9.,  5.],\n",
    "    [23., 31., 33., 31., 31., 21., 20., 48., 31., 22., 29., 28., 45., 38., 28., 17.],\n",
    "    [11.,  9., 12., 10.,  8., 10.,  6., 16., 13., 10.,  9., 11.,  7., 17., 11.,  4.],\n",
    "    [21., 20., 27., 28., 28., 15., 15., 38., 26., 20., 28., 23., 28., 34., 22., 11.],\n",
    "    [19., 18., 16., 17., 18., 12.,  6., 24., 12., 12., 14., 15., 22., 29., 20.,  6.],\n",
    "    [ 9., 10., 14., 10., 10.,  8.,  6., 15., 10.,  8.,  9., 13.,  9., 18., 11.,  4.],\n",
    "    [16., 15., 20., 17., 17.,  9., 14., 22., 16., 13., 16., 17., 16., 28., 14., 10.],\n",
    "    [15., 15., 19., 16., 12., 14.,  6., 25., 11., 12., 16., 12., 18., 27., 18.,  8.],\n",
    "    [12., 19., 19., 19., 16., 11.,  8., 24., 14., 12., 18., 12., 19., 30., 16.,  7.],\n",
    "    [15., 16., 20., 18., 16., 11., 11., 25., 16., 15., 15., 13., 23., 28., 16.,  6.],\n",
    "    [ 8., 11., 19., 11., 10., 11.,  8., 22., 13., 12., 16., 15.,  9., 19., 10.,  6.],\n",
    "    [ 7., 12., 12., 11., 10.,  9.,  5., 16., 12.,  8., 11., 11.,  9., 18., 9.,  4.],\n",
    "    [22., 19., 23., 25., 22., 15., 11., 29., 20., 17., 23., 19., 22., 34., 18., 13.],\n",
    "    [14., 19., 19., 23., 15., 18.,  9., 29., 16., 18., 17., 17., 24., 31., 18.,  5.],\n",
    "    [14., 15., 18., 16., 16., 10.,  7., 28., 15., 12., 16., 17., 17., 24., 17.,  6.],\n",
    "    [14., 15., 20., 24., 15., 14.,  8., 24., 16., 13., 14., 14., 22., 30., 16.,  9.],\n",
    "    [14., 15., 17., 18., 16., 14.,  8., 28., 15., 15., 15., 15., 20., 27., 16.,  7.],\n",
    "    [ 8.,  9., 15., 10.,  7., 10.,  6., 19., 13.,  8., 10., 11.,  9., 18., 11.,  4.],\n",
    "    [20., 22., 21., 23., 19., 15.,  8., 24., 13., 18., 15., 21., 26., 28., 22.,  9.],\n",
    "    [ 9., 10., 18., 11.,  9.,  8.,  6., 19., 13.,  8., 15., 13., 10., 18., 9.,  4.],\n",
    "    [ 8., 12., 13.,  9.,  8.,  8.,  5., 17., 12.,  8.,  9., 12.,  7., 18., 11.,  3.],\n",
    "    [15., 16., 18., 19., 10., 18.,  8., 20., 17., 12., 14., 18., 19., 22., 13.,  9.],\n",
    "    [14., 14., 16., 20., 16., 12.,  6., 28., 12., 18., 19., 13., 18., 27., 14.,  5.],\n",
    "    [20., 21., 30., 29., 23., 16., 16., 35., 30., 13., 27., 21., 27., 34., 23., 15.],\n",
    "    [13., 19., 15., 19., 15., 12.,  9., 24., 15., 13., 16., 17., 21., 24., 14.,  6.],\n",
    "    [16., 19., 18., 16., 13., 13.,  8., 27., 14., 12., 15., 15., 22., 27., 19.,  6.],\n",
    "    [15., 19., 23., 20., 17., 10.,  8., 30., 16., 15., 17., 20., 22., 26., 17.,  9.],\n",
    "    [12., 17., 19., 20., 14., 12.,  5., 25., 13., 13., 17., 16., 18., 24., 16.,  7.],\n",
    "    [14., 19., 22., 20., 16., 11.,  6., 26., 16., 13., 16., 15., 19., 28., 19.,  8.],\n",
    "    [15., 14., 17., 18., 19., 13., 11., 29., 18., 15., 16., 17., 19., 28., 15.,  8.],\n",
    "    [ 9., 12., 15., 11.,  9.,  8.,  3., 21., 10., 11., 13., 11.,  8., 20., 9.,  6.],\n",
    "    [ 9., 11., 11., 16.,  9.,  9.,  5., 20., 10., 12., 10., 13., 10., 20., 10.,  5.],\n",
    "    [17., 23., 22., 25., 19., 13., 11., 34., 18., 24., 19., 19., 24., 28., 21., 11.],\n",
    "    [10., 13., 13., 12., 12.,  8.,  4., 20., 12., 11., 13., 13., 10., 18., 10.,  5.],\n",
    "    [18., 19., 24., 20., 21., 14., 12., 34., 23., 20., 17., 17., 20., 28., 24., 13.],\n",
    "    [15., 13., 18., 12., 12.,  9.,  9., 20., 17., 11., 16., 11., 14., 20., 14.,  5.],\n",
    "    [ 8., 11., 16., 10., 10.,  9.,  3., 19., 10., 10., 10., 10., 10., 21., 11.,  4.],\n",
    "    [18., 20., 26., 29., 16., 18., 10., 37., 17., 19., 25., 14., 25., 35., 24., 15.],\n",
    "    [16., 17., 20., 21., 16.,  9.,  9., 30., 18., 12., 17., 18., 18., 26., 21.,  8.],\n",
    "    [ 8., 10., 12.,  9.,  9.,  9.,  5., 16., 11.,  8.,  9., 11., 10., 18., 8.,  3.],\n",
    "    [10., 10., 12., 13.,  7.,  8.,  4., 16., 12.,  7., 10., 11., 10., 18., 8.,  4.],\n",
    "    [ 9., 11., 18., 12., 11., 10.,  6., 17., 15.,  9., 14., 13.,  9., 19., 10.,  5.],\n",
    "    [ 8., 13., 15., 10.,  9.,  9.,  6., 16., 11.,  9., 11., 10., 11., 19., 8.,  7.],\n",
    "    [ 8., 13., 12., 13., 13., 11.,  5., 17., 12.,  8., 11., 15., 12., 21., 8.,  5.],\n",
    "    [ 9., 14., 14., 11., 12., 10.,  8., 18., 14.,  9., 11., 14., 14., 20., 10.,  4.],\n",
    "    [10., 10., 14., 12.,  8., 10.,  8., 20., 12., 13., 11., 11., 11., 18., 10.,  6.]\n",
    "])\n",
    "vectors = vectors.view(50,2,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89172d",
   "metadata": {},
   "source": [
    "# 4. 构造Pattern预测数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f25ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1224b1e",
   "metadata": {},
   "source": [
    "## 4.1 读取输出日志信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_samples = 10000\n",
    "pt_alpaca = torch.load(f'../alpaca_{num_samples}.pt')\n",
    "pt_yizhong = torch.load(f'../yizhong_{num_samples}.pt')\n",
    "len(pt_alpaca), len(pt_yizhong)\n",
    "pt = pt_alpaca + pt_yizhong\n",
    "seq_group = pt[0].outputs[0].seq_group\n",
    "seq = seq_group.get_seqs()[0]\n",
    "\n",
    "len(pt), seq_group.token2experts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331dc541",
   "metadata": {},
   "source": [
    "## 4.2 转化输出输出日子信息格式\n",
    "\n",
    "方便快速处理，格式示例如下：\n",
    "\n",
    "- 每个句子表示成dict格式\n",
    "- 每个句子记录 prompt_len 信息\n",
    "- 每个句子记录 data 信息，包含 prefilling 和 decoding 阶段的所有 token 和对应的 expert pattern 矩阵\n",
    "```python\n",
    "merged_data = [\n",
    "    {\n",
    "        'prompt_len': 23,\n",
    "        'data': [\n",
    "            (token_idx: int, one_hot_expert_pattern: np.array(32,8)),\n",
    "            (23,  大小为(32,8)的 one-hot 序列) # 1st token data\n",
    "            (423,  大小为(32,8)的 one-hot 序列) # 2nd token data\n",
    "            (273,  大小为(32,8)的 one-hot 序列) # 3rd token data\n",
    "            ...\n",
    "    \n",
    "        ]\n",
    "    } # 1st sequence data\n",
    "    {...}, # 2nd sequence data\n",
    "    {...}, # 3rd sequence data\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d815619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 sequence_group 数据转化成指点形式\n",
    "merged_data = [\n",
    "    # {\n",
    "    #     'prompt_len': 23,\n",
    "    #     'data': [\n",
    "    #         (token_idx: int, one_hot_expert_pattern: np.array(32,8)),\n",
    "    #         (23,  大小为(32,8)的 one-hot 序列) # 1st token data\n",
    "    #         (423,  大小为(32,8)的 one-hot 序列) # 2nd token data\n",
    "    #         (273,  大小为(32,8)的 one-hot 序列) # 3rd token data\n",
    "    #         ...\n",
    "    #\n",
    "    #     ]\n",
    "    # } # 1st sequence data\n",
    "    # {...}, # 2nd sequence data\n",
    "    # {...}, # 3rd sequence data\n",
    "    # ...\n",
    "]\n",
    "def convert_to_one_hot(array, max_value):\n",
    "    # Initialize a list with zeros of length max_value\n",
    "    one_hot_sequence = np.zeros(max_value, dtype=int)\n",
    "    one_hot_sequence[np.array(array)-1] = 1\n",
    "    return one_hot_sequence.tolist()\n",
    "\n",
    "for i in range(len(pt)):\n",
    "# for i in range(2):\n",
    "    seq_group = pt[i].outputs[0].seq_group\n",
    "    seq_group.token2experts\n",
    "    seq = seq_group.get_seqs()[0]\n",
    "    prompt_len = seq.get_prompt_len()\n",
    "    seq_data = []\n",
    "    for i_token in range(len(seq_group.token2experts)):\n",
    "        token_idx = seq_group.token2experts[i_token]['token_idx']\n",
    "        num_layers = len(seq_group.token2experts[i_token]) - 1\n",
    "        pattern = []\n",
    "        for layer_id in range(num_layers):\n",
    "            layer_pattern = seq_group.token2experts[i_token][layer_id][0]\n",
    "            one_hot_pattern = convert_to_one_hot(layer_pattern, max_value=8)\n",
    "            pattern.append(one_hot_pattern)\n",
    "        seq_data.append((token_idx, np.array(pattern)))\n",
    "    merged_data.append({\n",
    "        'prompt_len': prompt_len,\n",
    "        'data': seq_data\n",
    "    })\n",
    "torch.save(merged_data, 'merged_data.pt')\n",
    "len(merged_data),len(merged_data[0]['data']), merged_data[0]['data'][1][1].shape, merged_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a3f7d",
   "metadata": {},
   "source": [
    "如果已经生成过merge_data.pt，则直接读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d782211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = torch.load('merged_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25951428",
   "metadata": {},
   "source": [
    "## 4.3 训练数据集模板\n",
    "\n",
    "构建用来预测 pattern 的数据集，分成 prefilling 和 decoding 两套不同的prompt 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53365cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the inference prefilling phase of an MoE model with 32 layers and 8 experts per layer, each token's top-2 expert selections are identified, creating a (32, 2) expert selection pattern matrix. This matrix is pivotal for delineating the sentence's computational trajectory. Predict the comprehensive expert selection pattern of the sentence below: \n",
      "[\n",
      " prediction: [ childb lifelong \n",
      "]\n",
      "Prediction: \n",
      "\n",
      "\n",
      "During the inference decoding phase of an MoE model with 32 layers and 8 experts per layer, each token's top-2 expert selections are identified, creating a (32, 2) expert selection pattern matrix. This matrix is pivotal for delineating the token's computational trajectory. Predict the expert selection pattern for the final token in the sentence provided below:\n",
      "[\n",
      " prediction: [ childb lifelong \n",
      "]\n",
      "Prediction:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class MoEPromptGenerator:\n",
    "    def __init__(self, layers=32, experts_per_layer=8):\n",
    "        self.layers = layers\n",
    "        self.experts_per_layer = experts_per_layer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"/home/nus-hx/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/\")\n",
    "        self.prefilling_prompt_template = f\"\"\"During the inference prefilling phase of an MoE model with {self.layers} layers and {self.experts_per_layer} experts per layer, each token's top-2 expert selections are identified, creating a ({self.layers}, 2) expert selection pattern matrix. This matrix is pivotal for delineating the sentence's computational trajectory. Predict the comprehensive expert selection pattern of the sentence below: \\n[\n",
    "\"\"\"\n",
    "        self.decoding_prompt_template = f\"\"\"During the inference decoding phase of an MoE model with {self.layers} layers and {self.experts_per_layer} experts per layer, each token's top-2 expert selections are identified, creating a ({self.layers}, 2) expert selection pattern matrix. This matrix is pivotal for delineating the token's computational trajectory. Predict the expert selection pattern for the final token in the sentence provided below:\\n[\n",
    "\"\"\"\n",
    "        self.response_template = \"\\n]\\nPrediction:\"\n",
    "        self.prefilling_prompt_tokens = self.tokenizer.encode(self.prefilling_prompt_template)[1:]\n",
    "        self.decoding_prompt_tokens = self.tokenizer.encode(self.decoding_prompt_template)[1:]\n",
    "        self.response_tokens = self.tokenizer.encode(self.response_template)[1:]\n",
    "\n",
    "    def generate_prefilling_prompt_token(self, sentence_token):\n",
    "        return self.prefilling_prompt_tokens + sentence_token + self.response_tokens\n",
    "\n",
    "    def generate_decoding_prompt_token(self, sentence_token):\n",
    "        return self.decoding_prompt_tokens + sentence_token + self.response_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "# Example usage:\n",
    "prompt_generator = MoEPromptGenerator(32, 8)\n",
    "sentence_example = \"prediction: [ childb lifelong\"\n",
    "sentence_tokens = prompt_generator.tokenizer.encode(sentence_example)[1:]\n",
    "prefilling_prompt = prompt_generator.generate_prefilling_prompt_token(sentence_tokens)\n",
    "decoding_prompt = prompt_generator.generate_decoding_prompt_token(sentence_tokens)\n",
    "\n",
    "prefilling_text = prompt_generator.decode(prefilling_prompt)\n",
    "decoding_text = prompt_generator.decode(decoding_prompt)\n",
    "print(prefilling_text, '\\n\\n')\n",
    "print(decoding_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593bcc1",
   "metadata": {},
   "source": [
    "## 4.4 构建数据集\n",
    "\n",
    "数据集格式如下：\n",
    "\n",
    "```python\n",
    "pattern_dataset = [\n",
    "    {\n",
    "        \"type\": 'prefilling',\n",
    "        \"id\": sequence_index,\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": prefilling_text,  # 输入句子文本\n",
    "                \"token\": prefilling_tokens # 输入句子 token\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": pattern_matrix # 输出 pattern, 大小是 torch.size(32, 8)\n",
    "            }\n",
    "        ]\n",
    "    }, # 第一个数据\n",
    "    {...}, # 第二个数据\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f720664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = {\n",
    "#     'prompt_len': 23,\n",
    "#     'data': [\n",
    "#         (token_idx: int, one_hot_expert_pattern: np.array(32,8)),\n",
    "#         (23,  大小为(32,8)的 one-hot 序列) # 1st token data\n",
    "#         (423,  大小为(32,8)的 one-hot 序列) # 2nd token data\n",
    "#         (273,  大小为(32,8)的 one-hot 序列) # 3rd token data\n",
    "#         ...\n",
    "\n",
    "#     ]\n",
    "# }\n",
    "def get_expert_pattern(data_dict, start=0, end=None):\n",
    "    data = data_dict['data']\n",
    "    expert_pattern = np.zeros_like(data[0][1])\n",
    "    for i in range(start, end):\n",
    "        expert_pattern += data[i][1]\n",
    "    return expert_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6027b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "pattern_dataset = [\n",
    "    # {\n",
    "    #     \"type\": 'prefilling',\n",
    "    #     \"id\": sequence_index,\n",
    "    #     \"conversations\": [\n",
    "    #         {\n",
    "    #             \"from\": \"human\",\n",
    "    #             \"value\": prefilling_text,  # 输入句子文本\n",
    "    #             \"token\": prefilling_tokens # 输入句子 token\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"from\": \"gpt\",\n",
    "    #             \"value\": pattern_matrix # 输出 pattern, 大小是 torch.size(32, 8)\n",
    "    #         }\n",
    "    #     ]\n",
    "    # }, # 第一个数据\n",
    "    # {...}, # 第二个数据\n",
    "]\n",
    "seq_count = 0\n",
    "# for i in range(10):\n",
    "for i in range(len(merged_data)):\n",
    "    data_dict = merged_data[i]\n",
    "    \n",
    "    token_ids = [data_dict['data'][i][0] for i in range(len(data_dict['data']))]\n",
    "    prompt_len = data_dict['prompt_len']\n",
    "    all_len = len(token_ids)\n",
    "    prompt_ratio = prompt_len / all_len\n",
    "\n",
    "    for start_ratio in [0,0.1,0.2,0.3]:\n",
    "        for end_ratio in [prompt_ratio, 0.8,0.9,1]:\n",
    "            if start_ratio >= end_ratio:\n",
    "                continue\n",
    "            start = int(all_len * start_ratio)\n",
    "            end = int(all_len * end_ratio)\n",
    "            sentence_tokens = token_ids[start:end]\n",
    "            input_token_indices = prompt_generator.generate_prefilling_prompt_token(sentence_tokens)\n",
    "            tokens = prompt_generator.tokenizer.convert_ids_to_tokens(input_token_indices)\n",
    "            input_text = prompt_generator.tokenizer.convert_tokens_to_string(tokens)\n",
    "            # input_text = prompt_generator.decode(input_token_indices)\n",
    "            pattern = get_expert_pattern(data_dict, start=start, end=end)\n",
    "            assert end-start==pattern.sum(-1).mean()/2, f\"{end}!={pattern.sum(-1).mean()/2}\"\n",
    "            pattern_dataset.append({\n",
    "                \"type\": 'prefilling',\n",
    "                \"id\": seq_count,\n",
    "                \"conversations\": [\n",
    "                    {\n",
    "                        \"from\": \"human\",\n",
    "                        \"value\": input_text,  # 输入句子文本\n",
    "                        \"token\": input_token_indices # 输入句子 token\n",
    "                    },\n",
    "                    {\n",
    "                        \"from\": \"gpt\",\n",
    "                        \"value\": pattern # 输出 pattern, 大小是 torch.size(32, 8)\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "            seq_count += 1\n",
    "    if i % 5000 == 0:\n",
    "        print(i)\n",
    "        torch.save(pattern_dataset, 'pattern_dataset.pt')\n",
    "torch.save(pattern_dataset, 'pattern_dataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6e339",
   "metadata": {},
   "source": [
    "## 4.5 读取用于 pattern 预测的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d2eaa840",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_dataset = torch.load('pattern_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf82216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"During the inference prefilling phase of an MoE model with 32 layers and 8 experts per layer, each token's top-2 expert selections are identified, creating a (32, 2) expert selection pattern matrix. This matrix is pivotal for delineating the sentence's computational trajectory. Predict the comprehensive expert selection pattern of the sentence below: \\n[\\n<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDesign an application logo\\n\\n### Response: \\n]\\nPrediction:\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pattern_dataset))\n",
    "pattern_dataset[0]['conversations'][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b08aaaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6213, 272, 297, 2103, 710, 5806, 288, 6896, 302, 396, 6885, 28749, 2229, 395, 28705, 28770, 28750, 13083, 304, 28705, 28783, 11725, 660, 7487, 28725, 1430, 6029, 28742, 28713, 1830, 28733, 28750, 7583, 427, 4488, 460, 10248, 28725, 6818, 264, 325, 28770, 28750, 28725, 28705, 28750, 28731, 7583, 7495, 5340, 7111, 28723, 851, 7111, 349, 284, 449, 5834, 354, 882, 473, 1077, 272, 12271, 28742, 28713, 3633, 1249, 21699, 695, 28723, 19122, 848, 272, 15313, 7583, 7495, 5340, 302, 272, 12271, 3624, 28747, 28705, 13, 28792, 13, 3638, 28723, 12018, 264, 2899, 369, 6582, 1999, 2691, 274, 272, 2159, 28723, 13, 13, 27332, 3133, 3112, 28747, 13, 23342, 264, 1411, 7230, 13, 13, 27332, 12107, 28747, 13, 13, 2707, 1504, 1840, 5168, 304, 6485, 28723, 5372, 349, 264, 12734, 8123, 302, 1008, 28733, 2021, 8841, 304, 14204, 28723, 2929, 24256, 633, 9021, 28725, 8035, 3936, 28725, 304, 1484, 2115, 341, 16982, 298, 2727, 272, 1489, 2751, 302, 3936, 28723, 28705, 13, 28793, 13, 17555, 3033, 28747]\n",
      "[6213, 272, 297, 2103, 710, 5806, 288, 6896, 302, 396, 6885, 28749, 2229, 395, 28705, 28770, 28750, 13083, 304, 28705, 28783, 11725, 660, 7487, 28725, 1430, 6029, 28742, 28713, 1830, 28733, 28750, 7583, 427, 4488, 460, 10248, 28725, 6818, 264, 325, 28770, 28750, 28725, 28705, 28750, 28731, 7583, 7495, 5340, 7111, 28723, 851, 7111, 349, 284, 449, 5834, 354, 882, 473, 1077, 272, 12271, 28742, 28713, 3633, 1249, 21699, 695, 28723, 19122, 848, 272, 15313, 7583, 7495, 5340, 302, 272, 12271, 3624, 28747, 28705, 13, 28792, 13, 3638, 28723, 12018, 264, 2899, 369, 6582, 1999, 2691, 274, 272, 2159, 28723, 13, 13, 27332, 3133, 3112, 28747, 13, 23342, 264, 1411, 7230, 13, 13, 27332, 12107, 28747, 13, 13, 2707, 1504, 1840, 5168, 304, 6485, 28723, 5372, 349, 264, 12734, 8123, 302, 1008, 28733, 2021, 8841, 304, 14204, 28723, 2929, 24256, 633, 9021, 28725, 8035, 3936, 28725, 304, 1484, 2115, 341, 16982, 298, 2727, 272, 1489, 2751, 302, 3936, 28723, 28705, 13, 28793, 13, 17555, 3033, 28747]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "index = 22\n",
    "prefilling_text = pattern_dataset[index]['conversations'][0]['value']\n",
    "prefilling_token_indices = pattern_dataset[index]['conversations'][0]['token']\n",
    "crt_token_indices = tokenizer.encode(prefilling_text)\n",
    "print(prefilling_token_indices)\n",
    "print(crt_token_indices[1:])\n",
    "print(crt_token_indices[1:]==prefilling_token_indices)\n",
    "pattern_matrix = pattern_dataset[index]['conversations'][1]['value']\n",
    "# print(prefilling_text)\n",
    "# print('=====\\n\\n=====')\n",
    "tokens = tokenizer.convert_ids_to_tokens(prefilling_token_indices)\n",
    "text = tokenizer.convert_tokens_to_string(tokens)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "74392e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction childbirth lifelong huggingface\n",
      "['▁prediction', '▁child', 'b', 'irth', '▁lif', 'el', 'ong', '▁hug', 'ging', 'face']\n",
      "[19386, 1502, 28726, 4633, 5678, 301, 566, 13620, 3080, 1797]\n",
      "prediction childbirth lifelong huggingface\n",
      "['▁prediction', '▁child', 'b', 'irth', '▁lif', 'el', 'ong', '▁hug', 'ging', 'face']\n",
      "[19386, 1502, 28726, 4633, 5678, 301, 566, 13620, 3080, 1797]\n"
     ]
    }
   ],
   "source": [
    "origin_text = f\"\"\"prediction childbirth lifelong huggingface\"\"\"\n",
    "token_indices = tokenizer.encode(origin_text)[1:]\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_indices)\n",
    "text = tokenizer.convert_tokens_to_string(tokens)\n",
    "token_indices2 = tokenizer.encode(text)[1:]\n",
    "tokens2 = tokenizer.convert_ids_to_tokens(token_indices2)\n",
    "print(origin_text)\n",
    "print(tokens)\n",
    "print(token_indices)\n",
    "print(text)\n",
    "print(tokens2)\n",
    "print(token_indices2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80860cf8",
   "metadata": {},
   "source": [
    "## tokenizer测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f2b3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='/home/nus-hx/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "} <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "4 6\n",
      "0 3 [1, 28705, 28734]\n",
      "1 3 [1, 28705, 28740]\n",
      "2 3 [1, 28705, 28750]\n",
      "3 3 [1, 28705, 28770]\n",
      "4 3 [1, 28705, 28781]\n",
      "5 3 [1, 28705, 28782]\n",
      "6 3 [1, 28705, 28784]\n",
      "7 3 [1, 28705, 28787]\n",
      "8 3 [1, 28705, 28783]\n",
      "9 3 [1, 28705, 28774]\n",
      "123456789 11 [1, 28705, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
      "<s> 123456789\n",
      "\n",
      "Here is a design concept for your application logo:\n",
      "\n",
      "The logo features\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/nus-hx/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/\")\n",
    "print(tokenizer, type(tokenizer))\n",
    "inputs = \"Below is a childb\"\n",
    "token_indices = tokenizer.encode(inputs)\n",
    "print(len(inputs.split(\" \")), len(token_indices))\n",
    "for i in list(range(10))+[123456789]:\n",
    "    inputs = str(i)\n",
    "    token_indices = tokenizer.encode(inputs)\n",
    "    print(inputs, len(token_indices), token_indices)\n",
    "print(tokenizer.decode([1, 28705, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]))\n",
    "print(tokenizer.decode([13, 15423, 349, 264, 2621, 5935, 354, 574, 4993, 16388, 28747, 13, 13, 1014, 16388, 4190]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b85d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indices1 = torch.randint(0,20000,(30,))\n",
    "tokens = prompt_generator.tokenizer.convert_ids_to_tokens(token_indices1)\n",
    "token_string = prompt_generator.tokenizer.convert_tokens_to_string(tokens)\n",
    "reverse_token_indices = prompt_generator.tokenizer.encode(token_string)\n",
    "tokens2 = prompt_generator.tokenizer.convert_ids_to_tokens(reverse_token_indices)\n",
    "token_string2 = prompt_generator.tokenizer.convert_tokens_to_string(tokens2)\n",
    "print(' '.join(tokens))\n",
    "print(' '.join(tokens2))\n",
    "print(token_string)\n",
    "print(token_string2)\n",
    "print(token_indices1.numpy().tolist())\n",
    "print(reverse_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1000\n",
    "seq = pt[index].outputs[0].seq_group.get_seqs()[0]\n",
    "token_ids = seq.get_token_ids()\n",
    "prompt_len = seq.get_prompt_len()\n",
    "output_len = seq.get_output_len()\n",
    "assert output_len == len(token_ids) - prompt_len\n",
    "\n",
    "print('=====prompt=======')\n",
    "prompt_ids = token_ids[:prompt_len]\n",
    "prompt_tokens = tokenizer.convert_ids_to_tokens(prompt_ids, skip_special_tokens=False)\n",
    "prompt_token_string = tokenizer.convert_tokens_to_string(prompt_tokens)\n",
    "print(prompt_ids, '\\n====\\n', prompt_tokens, '\\n====\\n', prompt_token_string)\n",
    "\n",
    "print('=====decoding=======')\n",
    "output_ids = token_ids[prompt_len:]\n",
    "output_tokens = tokenizer.convert_ids_to_tokens(output_ids, skip_special_tokens=False)\n",
    "output_token_string = tokenizer.convert_tokens_to_string(output_tokens[:20])\n",
    "print(output_ids, '\\n====\\n', output_tokens, '\\n====\\n', output_token_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac00b86",
   "metadata": {},
   "source": [
    "## 。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159df0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
